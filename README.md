# AdaReasoner: Adaptive Reasoning Enables More Flexible Thinking

Official implementation of the paper:

> **AdaReasoner: Adaptive Reasoning Enables More Flexible Thinking**
> Xiangqi Wang, Yue Huang, Yanbo Wang, Xiaonan Luo, Kehan Guo, Yujun Zhou, Xiangliang Zhang
> University of Notre Dame & MBZUAI
> ğŸ“§ Contact: [xwang76@nd.edu](mailto:xwang76@nd.edu)

---

## ğŸ” Overview

**AdaReasoner** is a lightweight, reinforcement learningâ€“based plugin designed to dynamically adapt reasoning configurations for Large Language Models (LLMs). Unlike traditional prompting methods that rely on fixed settings, AdaReasoner enables LLMs to adjust reasoning instructions, temperature, and step count based on the question type â€” thereby achieving **more flexible and task-specific thinking**.

This repository provides the official codebase for training, evaluating, and running AdaReasoner across various reasoning tasks.

---

## ğŸ§  Key Features

- âœ… **LLM-agnostic** design â€” works with GPT-4, Claude, LLaMA, Qwen, and more.
- ğŸ¯ **Reinforcement Learning core** â€” learns optimal configuration using reward-guided training.
- ğŸ“Š **Few-shot efficient** â€” achieves high performance with as few as 50â€“100 labeled samples.
- ğŸ§© **Composable instruction space** â€” builds instructions from base + variation pairs.
- ğŸ† **Outperforms standard baselines** â€” including CoT, ToT, Auto-CoT, Best-of-N, and more.

---

## ğŸ“ Repository Structure


OFFICIAL_ADAREASONER/

â”œâ”€â”€ baselines/                # Baseline reasoning methods (e.g., CoT, ToT)

â”œâ”€â”€ dataset/                 # Contains source.json and data loaders

â”œâ”€â”€ eval/                    # Evaluation utilities of all baselines

â”œâ”€â”€ module/                  # Core RL policy implementation for AdaReasoner

â”œâ”€â”€ adatrain.py              # Train AdaReasoner from scratch

â”œâ”€â”€ adatest.py               # Run inference using pretrained policy (gpt4oadapt.pkl)

â”œâ”€â”€ baseline_evaluation.py   # Compare with baseline strategies

â”œâ”€â”€ statistic.py             # Compute and visualize statistics

â”œâ”€â”€ gpt4oadapt.pkl           # Pretrained AdaReasoner weights for GPT-4o

â”œâ”€â”€ requirements.txt         # Python dependencies (auto-generated)

â””â”€â”€ README.md                # This file


## ğŸš€ Quick Start

### 1. Setup Environment

Install dependencies (see notes below for minimal dependency extraction):

```bash
pip install -r requirements.txt
```

### 2. Run Demo Inference

```python
python adatest.py

```

* This will load the pretrained model from `gpt4oadapt.pkl`.
* Evaluation is performed on the test sets included in `dataset/source.json`, which contains a mix of:
  * ğŸ§® MMLU (Math)
  * ğŸ§  Metaphor Classification
  * ğŸ” LogiQA (Logical Reasoning)
  * ğŸ§¾ TruthfulQA (Factuality & Trust)

### Train Your Own Adapter

To train AdaReasoner on a new or existing dataset:

```python
python adatrain.py
```

* Reward model: by default, a DeBERTa-based reward model is used (see paper Appendix for details).
* Configuration space: consists of generation temperature, reasoning steps, and instruction formats.

## ğŸ“Š Benchmarks

AdaReasoner has been evaluated across **six LLMs** and multiple benchmark datasets. It consistently outperforms existing baselines in both in-domain and out-of-distribution reasoning tasks.

For example, using GPT-4o:

| Method                | Metaphor        | TruthfulQA      | MMLU (Math)     | LogiQA          | Average         |
| --------------------- | --------------- | --------------- | --------------- | --------------- | --------------- |
| CoT                   | 50.40           | 78.40           | 76.04           | 70.00           | 68.71           |
| Auto-CoT              | 62.33           | 83.09           | 72.15           | 71.71           | 72.32           |
| **AdaReasoner** | **71.56** | **81.30** | **86.49** | **82.31** | **80.42** |

## ğŸ“„ Citation

If you find this work helpful, please consider citing:

....
